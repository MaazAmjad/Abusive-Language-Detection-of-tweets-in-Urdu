{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Sat Oct 12 12:47:21 2019\\n\\n@author: Noman Ashraf and Maaz Amjad\\n\\n@code: Abusive Language Classification\\n\\n@paper: Hairy at the Heel: Abusive Language Detection in Urdu Tweets with Analysis of Count-Based and Embedding-Based Features\\n\\n'"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sat Oct 12 12:47:21 2019\n",
    "\n",
    "@author: Noman Ashraf and Maaz Amjad\n",
    "\n",
    "@code: Abusive Language Classification\n",
    "\n",
    "@paper: Hairy at the Heel: Abusive Language Detection in Urdu Tweets with Analysis of Count-Based and Embedding-Based Features\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Import adjustText, initialize list of texts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score,fbeta_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models.fasttext import FastText, load_facebook_vectors\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras import backend\n",
    "from keras import backend as K\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM, Dropout, Flatten, Embedding, Bidirectional, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling2D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "from keras.regularizers import l2,l1\n",
    "\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "#Seed Random Numbers with the TensorFlow Backend\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"train_ds.csv\"\n",
    "test_data = \"test_ds.csv\"\n",
    "w2v_file_fast_text = \"cc.ur.300.bin.gz\"\n",
    "w2v_file_glove = \"glove_model2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df_train = pd.read_csv(train_data, usecols=['Tweets', 'label'])\n",
    "#df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "#df_train.to_csv(\"ds.csv\", sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "df_train.head()\n",
    "rows,col = df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بھگوڑی لیگ پر پورے کشمیروں کی لعنت</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ہجڑے کو ہجڑا نا کہوں تو کیا کہوں دختر زرداری</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>گشتی کے بچے، دروازے کی چابیاں تیرے باپ باجوہ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>کسی گشتی کے پتر ہیں یہ حرامی کافر مادرچود</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ماں سے پوچھ کتی کے بچے، کس گشتے سے مروا کر آئ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  label\n",
       "0                بھگوڑی لیگ پر پورے کشمیروں کی لعنت       1\n",
       "1      ہجڑے کو ہجڑا نا کہوں تو کیا کہوں دختر زرداری       1\n",
       "2   گشتی کے بچے، دروازے کی چابیاں تیرے باپ باجوہ ...      1\n",
       "3        کسی گشتی کے پتر ہیں یہ حرامی کافر مادرچود        1\n",
       "6   ماں سے پوچھ کتی کے بچے، کس گشتے سے مروا کر آئ...      1"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeswisedf = df_train[(df_train['label'] == 1)]\n",
    "yeswisedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>عبدالقدیر خان نے کہا کارگل جنگ میں ہم دہلی صرف...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>تو پھر ایک جرنل کے تین سال پر اتنی بکواس کیوں؟</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>کراچی میں پانچ سے دس لوگ بارشوں کی وجہ سے جاں ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>یہ تو اس وقت سوچنا چاہییے تھا جب عوام کو دونوں...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>گرین ٹاؤن ستو کتلہ کی فردوس کوثر بی بی جس کے م...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets  label\n",
       "4   عبدالقدیر خان نے کہا کارگل جنگ میں ہم دہلی صرف...      0\n",
       "5     تو پھر ایک جرنل کے تین سال پر اتنی بکواس کیوں؟       0\n",
       "9   کراچی میں پانچ سے دس لوگ بارشوں کی وجہ سے جاں ...      0\n",
       "10  یہ تو اس وقت سوچنا چاہییے تھا جب عوام کو دونوں...      0\n",
       "11  گرین ٹاؤن ستو کتلہ کی فردوس کوثر بی بی جس کے م...      0"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nowisedf = df_train[df_train['label'] == 0]\n",
    "nowisedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row = pd.concat([yeswisedf['Tweets'], nowisedf['Tweets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  بھگوڑی لیگ پر پورے کشمیروں کی لعنت \n",
       "1        ہجڑے کو ہجڑا نا کہوں تو کیا کہوں دختر زرداری \n",
       "2     گشتی کے بچے، دروازے کی چابیاں تیرے باپ باجوہ ...\n",
       "3          کسی گشتی کے پتر ہیں یہ حرامی کافر مادرچود  \n",
       "6     ماں سے پوچھ کتی کے بچے، کس گشتے سے مروا کر آئ...\n",
       "Name: Tweets, dtype: object"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_row.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Dataset:2800\n",
      "yes class:1400\n",
      "no class:1400\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Training Dataset:\" + str(len(df_row)))\n",
    "print(\"yes class:\" + str(len(yeswisedf)))\n",
    "print(\"no class:\" + str(len(nowisedf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_data, usecols=['Tweets', 'label'])\n",
    "#df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "#df_test.to_csv(\"ds.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Dataset Abusive ===========\n",
      "(1750, 4)\n",
      "        word_count     char_count\n",
      "sum   26378.000000  118512.000000\n",
      "mean     15.073143      67.721143\n",
      "=========== Dataset Abusive ===========\n"
     ]
    }
   ],
   "source": [
    "abusive_df = df_full[(df_full['label'] == 1)]\n",
    "abusive_df['word_count'] = abusive_df['Tweets'].apply(lambda x: len(str(x).split()))\n",
    "abusive_df['char_count'] = abusive_df['Tweets'].str.len() ## this also includes spaces\n",
    "\n",
    "\n",
    "df_abusive_stat = abusive_df[['word_count', 'char_count']].agg(['sum','mean'])\n",
    "\n",
    "print(\"=========== Dataset Abusive ===========\")\n",
    "print(abusive_df.shape)\n",
    "print(df_abusive_stat)\n",
    "print(\"=========== Dataset Abusive ===========\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Dataset non-Abusive ===========\n",
      "(1750, 4)\n",
      "      word_count     char_count\n",
      "sum    30709.000  140627.000000\n",
      "mean      17.548      80.358286\n",
      "=========== Dataset non-Abusive ===========\n"
     ]
    }
   ],
   "source": [
    "non_abusive_df = df_full[(df_full['label'] == 0)]\n",
    "non_abusive_df['word_count'] = non_abusive_df['Tweets'].apply(lambda x: len(str(x).split()))\n",
    "non_abusive_df['char_count'] = non_abusive_df['Tweets'].str.len() ## this also includes spaces\n",
    "\n",
    "\n",
    "df_non_abusive_stat = non_abusive_df[['word_count', 'char_count']].agg(['sum','mean'])\n",
    "\n",
    "print(\"=========== Dataset non-Abusive ===========\")\n",
    "print(non_abusive_df.shape)\n",
    "print(df_non_abusive_stat)\n",
    "print(\"=========== Dataset non-Abusive ===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'بھگوڑی', 'لیگ', 'پر', 'پورے', 'کشمیروں', 'کی', 'لعنت', '']\n"
     ]
    }
   ],
   "source": [
    "keywords_dictionary = []\n",
    "sentences_corpus = []\n",
    "for index,row in df_full.iterrows():\n",
    "    text = str(row['Tweets'])            \n",
    "    sentences_corpus.append(text)\n",
    "    list_of_words = text.split(\" \")\n",
    "    keywords_dictionary.append(list_of_words)\n",
    "            \n",
    "print(keywords_dictionary[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for kl in keywords_dictionary: \n",
    "    for w in kl:\n",
    "        vocab.append(str(w))\n",
    "            \n",
    "corpus = []\n",
    "for kl in sentences_corpus: \n",
    "    corpus = corpus + kl.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = True \n",
    "#words = False \n",
    "#for char ...use words = False \n",
    "_ngram_range = (2,2)\n",
    "_max_features = 100\n",
    "#_max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "if words:\n",
    "    vectorizer = CountVectorizer(ngram_range=_ngram_range, max_features=_max_features)\n",
    "    Count_Vect = vectorizer.fit_transform(corpus)\n",
    "else:\n",
    "    vectorizer = CountVectorizer(ngram_range=_ngram_range, token_pattern = r\"(?u)\\b\\w+\\b\",  analyzer='char')\n",
    "    Count_Vect = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=_ngram_range,max_features=_max_features) # You can still specify n-grams here.\n",
    "X = vectorizer.fit_transform(corpus).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final Ngram vector:(3500, 100)\n"
     ]
    }
   ],
   "source": [
    "print( \"Shape of final Ngram vector:\" + str(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shape represents... 3500 == total rows\n",
    "#### 2nd element == # of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, yTrain = X[:rows], df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest, yTest = X[rows:], df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train vector:(2800, 100)\n",
      "Shape of train labels:(2800,)\n"
     ]
    }
   ],
   "source": [
    "print( \"Shape of train vector:\" + str(xTrain.shape))\n",
    "print( \"Shape of train labels:\" + str(yTrain.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test vector:(700, 100)\n",
      "Shape of test labels:(700,)\n"
     ]
    }
   ],
   "source": [
    "print( \"Shape of test vector:\" + str(xTest.shape))\n",
    "print( \"Shape of test labels:\" + str(yTest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shape_Reshaper(_xTrain, _xTest):    \n",
    "    xTrain_arr = _xTrain\n",
    "    xTest_arr = _xTest\n",
    "\n",
    "\n",
    "    dim1,dim3 = _xTrain.shape\n",
    "    dim2 = 1        \n",
    "\n",
    "    xTrain = np.reshape(_xTrain, (dim1, dim2 , dim3))\n",
    "    print(xTrain.shape)       \n",
    "\n",
    "    t_dim1,t_dim3 = _xTest.shape\n",
    "    t_dim2 = 1\n",
    "    xTest = xTest_arr.reshape(t_dim1, t_dim2 , t_dim3)\n",
    "    print(xTest.shape)\n",
    "\n",
    "    _input_shape = (dim2,dim3)\n",
    "    print(_input_shape)\n",
    "\n",
    "\n",
    "    return xTrain,xTest,_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_Ngrams(xTrain, xTest, yTrain, yTest, _C = [0.1, 1, 10], _kernel= ['linear', 'rbf'], _tol = [0.01, 0.02], _class_weight = 'balanced'):\n",
    "    print(\"--------------- Support Vector Machine ---------------\")\n",
    "    print(\"Fitting the classifier to the training set\")\n",
    "    param_grid = {'C': _C, 'kernel': _kernel, 'tol': _tol}\n",
    "    clf = GridSearchCV(svm.SVC(probability=True,class_weight='balanced'), param_grid)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    y_pred = clf.predict(xTest)\n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, y_pred)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "\n",
    "\n",
    "    print(\"=== Classification Report SVM ===\")\n",
    "    print(classification_report(yTest, y_pred))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix SVM ===\")\n",
    "    print(confusion_matrix(yTest, y_pred))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"=== ROC Score SVM ===\")\n",
    "    print(roc_auc_score(yTest, y_pred))\n",
    "    \n",
    "    \n",
    "    print(\"--------------- Parameters ---------------\")\n",
    "    print_results(clf)\n",
    "    \n",
    "    \n",
    "    print(\"--------------- Support Vector Machine ---------------\")\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_Ngrams(xTrain, xTest, yTrain, yTest, _n_jobs=3, _criterion = 'gini', _verbose=10):\n",
    "    print(\"--------------- Random Forest ---------------\")\n",
    "    \n",
    "    rfc = RandomForestClassifier( max_features=None,  n_jobs=_n_jobs, criterion = _criterion)\n",
    "    parameters = {\n",
    "        'n_estimators': [5,50,250],\n",
    "        'max_depth': [10, 15, 20, 30]\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(rfc, parameters)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    \n",
    "    print(clf)\n",
    "    print(\"Predicting labels for test data..\")\n",
    "    rfc_predict  = clf.predict(xTest)\n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, rfc_predict)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print(\"=== Classification Report Random Forest ===\")\n",
    "    print(classification_report(yTest, rfc_predict))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix Random Forest ===\")\n",
    "    print(confusion_matrix(yTest, rfc_predict))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"=== ROC Score RF ===\")\n",
    "    print(roc_auc_score(yTest, rfc_predict))\n",
    "    \n",
    "    print(\"--------------- Parameters ---------------\")\n",
    "    print_results(clf)\n",
    "    \n",
    "    print(\"--------------- Random Forest ---------------\")\n",
    "    return rfc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GB_Ngrams(xTrain, xTest, yTrain, yTest):\n",
    "#     print(\"--------------- Gradient Boosting ---------------\")\n",
    "    \n",
    "#     gb = GradientBoostingClassifier()\n",
    "#     parameters = {\n",
    "#         'n_estimators': [5, 50, 250, 500],\n",
    "#         'max_depth': [1, 3, 5, 7, 9],\n",
    "#         'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "#     }\n",
    "\n",
    "#     cv = GridSearchCV(gb, parameters, cv=5)\n",
    "#     cv.fit(xTrain, yTrain)\n",
    "#     print(\"Predicting labels for test data..\")\n",
    "#     gbc_predict  = gb.predict(xTest)\n",
    "\n",
    "#     print(\"=== Classification Report Gradient Boosting  ===\")\n",
    "#     print(classification_report(yTest, gbc_predict))\n",
    "#     print('\\n')\n",
    "#     print(\"=== Confusion Matrix Gradient Boosting ===\")\n",
    "#     print(confusion_matrix(yTest, gbc_predict))\n",
    "#     print('\\n')\n",
    "#     print(\"--------------- Gradient Boosting ---------------\")\n",
    "#     return gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes_Ngrams(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"--------------- Naive Bayes ---------------\")      \n",
    "    model =  GaussianNB()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    predictions = model.predict(xTest)\n",
    "\n",
    "    \n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, predictions)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print(\"=== Classification Report Naive Bayes ===\")\n",
    "    print(classification_report(yTest, predictions))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix Naive Bayes ===\")\n",
    "    print(confusion_matrix(yTest, predictions))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"=== ROC Score Naive Bayes ===\")\n",
    "    print(roc_auc_score(yTest, predictions, average='weighted'))\n",
    "    \n",
    "   \n",
    "    print(\"--------------- Naive Bayes ---------------\")\n",
    "    return model\n",
    "\n",
    "#print(\"=== ROC Curve Naive Bayes ===\")\n",
    "# fpr, tpr, thresholds = roc_curve(yTest, predictions, pos_label=None)\n",
    "#     print(fpr)\n",
    "#     print(tpr)\n",
    "#     print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_Ngrams(xTrain, xTest, yTrain, yTest, _n_estimators = 100, _n_jobs=3, _criterion = 'gini', _max_depth  = 15, _verbose=10):\n",
    "    print(\"--------------- Decision Tree ---------------\")\n",
    "    \n",
    "    dt = DecisionTreeClassifier(random_state=0)\n",
    "    dt = dt.fit(xTrain, yTrain)\n",
    "    print(\"Predicting labels for test data..\")\n",
    "    dt_predict  = dt.predict(xTest)\n",
    "\n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, dt_predict)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print(\"=== Classification Decision Tree ===\")\n",
    "    print(classification_report(yTest, dt_predict))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix Decision Tree ===\")\n",
    "    print(confusion_matrix(yTest, dt_predict))\n",
    "    \n",
    "    \n",
    "    print(\"=== ROC Score DT ===\")\n",
    "    print(roc_auc_score(yTest, dt_predict))\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"--------------- Decision Tree ---------------\")\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost_Ngrams(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"--------------- Adaboost ---------------\")\n",
    "    \n",
    "    ad = AdaBoostClassifier()\n",
    "    parameters = {\n",
    "        'n_estimators': [5, 50, 250, 500]\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(ad, parameters, cv=5)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "\n",
    "    print(\"Predicting labels for test data..\")\n",
    "    ad_predict  = clf.predict(xTest)\n",
    "\n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, ad_predict)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print(\"=== Classification Adaboost ===\")\n",
    "    print(classification_report(yTest, ad_predict))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix Adaboost ===\")\n",
    "    print(confusion_matrix(yTest, ad_predict))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"=== ROC Score AD ===\")\n",
    "    print(roc_auc_score(yTest, ad_predict))\n",
    "    \n",
    "    \n",
    "    print(\"--------------- Parameters ---------------\")\n",
    "    print_results(clf)\n",
    "    \n",
    "    print(\"--------------- Adaboost ---------------\")\n",
    "    return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_Ngrams(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"--------------- Logistic Regression ---------------\")\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    parameters = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(lr, parameters, cv=5)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    \n",
    "    print(\"Predicting labels for test data..\")\n",
    "    lr_predict  = clf.predict(xTest)\n",
    "\n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, lr_predict)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print(\"=== Classification Logistic Regression ===\")\n",
    "    print(classification_report(yTest, lr_predict))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix Logistic Regression ===\")\n",
    "    print(confusion_matrix(yTest, lr_predict))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"=== ROC Score LR ===\")\n",
    "    print(roc_auc_score(yTest, lr_predict))\n",
    "    \n",
    "    print(\"--------------- Parameters ---------------\")\n",
    "    print_results(clf)\n",
    "    \n",
    "    print(\"--------------- Logistic Regression ---------------\")\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Random Forest ---------------\n",
      "GridSearchCV(cv=None, error_score=nan,\n",
      "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                                              class_weight=None,\n",
      "                                              criterion='gini', max_depth=None,\n",
      "                                              max_features=None,\n",
      "                                              max_leaf_nodes=None,\n",
      "                                              max_samples=None,\n",
      "                                              min_impurity_decrease=0.0,\n",
      "                                              min_impurity_split=None,\n",
      "                                              min_samples_leaf=1,\n",
      "                                              min_samples_split=2,\n",
      "                                              min_weight_fraction_leaf=0.0,\n",
      "                                              n_estimators=100, n_jobs=3,\n",
      "                                              oob_score=False,\n",
      "                                              random_state=None, verbose=0,\n",
      "                                              warm_start=False),\n",
      "             iid='deprecated', n_jobs=None,\n",
      "             param_grid={'max_depth': [10, 15, 20, 30],\n",
      "                         'n_estimators': [5, 50, 250]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring=None, verbose=0)\n",
      "Predicting labels for test data..\n",
      "=== AUC Score ===\n",
      "Accuracy: 0.642857\n",
      "=== Classification Report Random Forest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.90      0.72       350\n",
      "           1       0.79      0.39      0.52       350\n",
      "\n",
      "    accuracy                           0.64       700\n",
      "   macro avg       0.69      0.64      0.62       700\n",
      "weighted avg       0.69      0.64      0.62       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix Random Forest ===\n",
      "[[314  36]\n",
      " [214 136]]\n",
      "\n",
      "\n",
      "=== ROC Score RF ===\n",
      "0.6428571428571429\n",
      "--------------- Parameters ---------------\n",
      "BEST PARAMS: {'max_depth': 30, 'n_estimators': 50}\n",
      "\n",
      "0.638 (+/-0.019) for {'max_depth': 10, 'n_estimators': 5}\n",
      "0.65 (+/-0.018) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.652 (+/-0.016) for {'max_depth': 10, 'n_estimators': 250}\n",
      "0.648 (+/-0.007) for {'max_depth': 15, 'n_estimators': 5}\n",
      "0.654 (+/-0.009) for {'max_depth': 15, 'n_estimators': 50}\n",
      "0.654 (+/-0.016) for {'max_depth': 15, 'n_estimators': 250}\n",
      "0.655 (+/-0.01) for {'max_depth': 20, 'n_estimators': 5}\n",
      "0.656 (+/-0.008) for {'max_depth': 20, 'n_estimators': 50}\n",
      "0.656 (+/-0.01) for {'max_depth': 20, 'n_estimators': 250}\n",
      "0.655 (+/-0.023) for {'max_depth': 30, 'n_estimators': 5}\n",
      "0.662 (+/-0.012) for {'max_depth': 30, 'n_estimators': 50}\n",
      "0.661 (+/-0.017) for {'max_depth': 30, 'n_estimators': 250}\n",
      "--------------- Random Forest ---------------\n"
     ]
    }
   ],
   "source": [
    "rf_ngram = RF_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Logistic Regression ---------------\n",
      "Predicting labels for test data..\n",
      "=== AUC Score ===\n",
      "Accuracy: 0.835714\n",
      "=== Classification Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       350\n",
      "           1       0.86      0.80      0.83       350\n",
      "\n",
      "    accuracy                           0.84       700\n",
      "   macro avg       0.84      0.84      0.84       700\n",
      "weighted avg       0.84      0.84      0.84       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix Logistic Regression ===\n",
      "[[305  45]\n",
      " [ 70 280]]\n",
      "\n",
      "\n",
      "=== ROC Score LR ===\n",
      "0.8357142857142857\n",
      "--------------- Parameters ---------------\n",
      "BEST PARAMS: {'C': 1}\n",
      "\n",
      "0.809 (+/-0.054) for {'C': 0.001}\n",
      "0.809 (+/-0.053) for {'C': 0.01}\n",
      "0.817 (+/-0.051) for {'C': 0.1}\n",
      "0.821 (+/-0.035) for {'C': 1}\n",
      "0.804 (+/-0.024) for {'C': 10}\n",
      "0.774 (+/-0.048) for {'C': 100}\n",
      "0.753 (+/-0.033) for {'C': 1000}\n",
      "--------------- Logistic Regression ---------------\n"
     ]
    }
   ],
   "source": [
    "lf_ngram = LR_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Adaboost ---------------\n",
      "Predicting labels for test data..\n",
      "=== AUC Score ===\n",
      "Accuracy: 0.808571\n",
      "=== Classification Adaboost ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83       350\n",
      "           1       0.88      0.71      0.79       350\n",
      "\n",
      "    accuracy                           0.81       700\n",
      "   macro avg       0.82      0.81      0.81       700\n",
      "weighted avg       0.82      0.81      0.81       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix Adaboost ===\n",
      "[[317  33]\n",
      " [101 249]]\n",
      "\n",
      "\n",
      "=== ROC Score AD ===\n",
      "0.8085714285714285\n",
      "--------------- Parameters ---------------\n",
      "BEST PARAMS: {'n_estimators': 50}\n",
      "\n",
      "0.689 (+/-0.035) for {'n_estimators': 5}\n",
      "0.794 (+/-0.016) for {'n_estimators': 50}\n",
      "0.785 (+/-0.02) for {'n_estimators': 250}\n",
      "0.768 (+/-0.032) for {'n_estimators': 500}\n",
      "--------------- Adaboost ---------------\n"
     ]
    }
   ],
   "source": [
    "adb_ngram =  AdaBoost_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Naive Bayes ---------------\n",
      "=== AUC Score ===\n",
      "Accuracy: 0.721429\n",
      "=== Classification Report Naive Bayes ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.53      0.65       350\n",
      "           1       0.66      0.91      0.77       350\n",
      "\n",
      "    accuracy                           0.72       700\n",
      "   macro avg       0.76      0.72      0.71       700\n",
      "weighted avg       0.76      0.72      0.71       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix Naive Bayes ===\n",
      "[[185 165]\n",
      " [ 30 320]]\n",
      "\n",
      "\n",
      "=== ROC Score Naive Bayes ===\n",
      "0.7214285714285714\n",
      "--------------- Naive Bayes ---------------\n"
     ]
    }
   ],
   "source": [
    "nb_ngram = Naive_Bayes_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Decision Tree ---------------\n",
      "Predicting labels for test data..\n",
      "=== AUC Score ===\n",
      "Accuracy: 0.784286\n",
      "=== Classification Decision Tree ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       350\n",
      "           1       0.79      0.77      0.78       350\n",
      "\n",
      "    accuracy                           0.78       700\n",
      "   macro avg       0.78      0.78      0.78       700\n",
      "weighted avg       0.78      0.78      0.78       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix Decision Tree ===\n",
      "[[280  70]\n",
      " [ 81 269]]\n",
      "=== ROC Score DT ===\n",
      "0.7842857142857144\n",
      "\n",
      "\n",
      "--------------- Decision Tree ---------------\n"
     ]
    }
   ],
   "source": [
    "dt_ngram = DT_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Support Vector Machine ---------------\n",
      "Fitting the classifier to the training set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-515-53e910dba1e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvm_ngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM_Ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-503-5eba462720a6>\u001b[0m in \u001b[0;36mSVM_Ngrams\u001b[0;34m(xTrain, xTest, yTrain, yTest, _C, _kernel, _tol, _class_weight)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tol'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tol\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best estimator found by grid search:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svm_ngram = SVM_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb_ngram =  GB_Ngrams(xTrain, xTest, yTrain, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 Vector File Loaded!\n"
     ]
    }
   ],
   "source": [
    "#%% fast load\n",
    "w2vmodel = FastText.load_fasttext_format(w2v_file_fast_text)   \n",
    "print(\"Word 2 Vector File Loaded!\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLOVE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 Vector File Loaded!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#%% Glove load\n",
    "w2vmodel = KeyedVectors.load_word2vec_format(w2v_file_glove, binary=False)\n",
    "#w2vmodel = KeyedVectors.load_word2vec_format(w2v_file_glove)\n",
    "print(\"Word 2 Vector File Loaded!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Ngrams(_xTrain, _xTest, yTrain, yTest, _loss='mean_squared_error', _optimizer= 'Adam', _metrics=['accuracy'], _epochs = 25 , _validation_split = 0.2, _batch_size = 2, _verbose = 0 ):\n",
    "    print(\"--------------- LSTM ---------------\")       \n",
    "\n",
    "    xTrain,xTest,_input_shape = Shape_Reshaper(_xTrain, _xTest)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=_input_shape, activation='tanh' , return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(1 ,activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss=_loss, optimizer= _optimizer, metrics=_metrics)\n",
    "    history = model.fit(xTrain, yTrain, epochs=_epochs,  validation_split = _validation_split)\n",
    "\n",
    "    accuracy = model.evaluate(xTest, yTest,  verbose= _verbose)\n",
    "    y_pred = model.predict_classes(xTest, batch_size = _batch_size, verbose = _verbose)\n",
    "    \n",
    "    print(\"=== AUC Score ===\")\n",
    "    accuracy = accuracy_score(yTest, y_pred)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "\n",
    "    print(\"=== Classification Report LSTM ===\")\n",
    "    print(classification_report(yTest, y_pred))\n",
    "    print('\\n')\n",
    "    print(\"=== Confusion Matrix LSTM ===\")\n",
    "    print(confusion_matrix(yTest, y_pred))\n",
    "    print('\\n')\n",
    "        \n",
    "    print(\"=== ROC Score Naive Bayes ===\")\n",
    "    print(roc_auc_score(yTest, y_pred, average='weighted'))\n",
    "    \n",
    "    print(\"--------------- LSTM ---------------\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Vectors(data):\n",
    "    vector = w2vmodel.wv['easy']\n",
    "    print( \"Shape of Vector:\" + str(vector.shape))\n",
    "\n",
    " \n",
    "    X_train_Vector = []\n",
    "    for d in data:\n",
    "        words = sent_tokenize(d) \n",
    "        vector_list = []\n",
    "        for word in words:\n",
    "            if word in w2vmodel.wv.vocab:\n",
    "                vector_list.append(w2vmodel[word])\n",
    "            else:\n",
    "                vector_list.append(np.random.uniform(-0.1, 0.1, 300))\n",
    "\n",
    "        matrix_2d = np.array(vector_list)\n",
    "        average_sentence_vector = np.mean(matrix_2d, axis = 0)\n",
    "        X_train_Vector.append(average_sentence_vector)\n",
    "\n",
    "    X = numpy.array(X_train_Vector)\n",
    "    print( \"Shape of documents\" + str(X.shape)) \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1D_Ngrams(_xTrain, _xTest, yTrain, yTest, _loss='mean_squared_error', _optimizer= 'Adam', _metrics=['accuracy'], _epochs = 25 , _validation_split = 0.2, _batch_size = 4, _verbose = 2 ):\n",
    "    print(\"--------------- CNN1D ---------------\")  \n",
    "    \n",
    "    xTrain,xTest,_input_shape = Shape_Reshaper(_xTrain, _xTest)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(16, (3),strides=3, padding='same',activation='tanh', input_shape=_input_shape))\n",
    "    model.add(Dense(32 ,activation='tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    #model.add(Dense(64 ,activation='tanh'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(1 ,activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss=_loss, optimizer=_optimizer, metrics=_metrics)\n",
    "\n",
    "    model.fit(xTrain, yTrain, epochs=_epochs,  validation_split=_validation_split)\n",
    "\n",
    "    accuracy = model.evaluate(xTest, yTest,  verbose=_verbose)\n",
    "    print(accuracy)\n",
    "    y_pred = model.predict_classes(xTest)\n",
    "\n",
    "    print(\"=== Classification Report Conv 1D ===\")\n",
    "    print(classification_report(yTest, y_pred))\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"=== Confusion Matrix Conv 1D ===\")\n",
    "    print(confusion_matrix(yTest, y_pred))\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"--------------- CNN1D ---------------\") \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Vector:(300,)\n",
      "Shape of documents(2800, 300)\n"
     ]
    }
   ],
   "source": [
    "xTrain = Generate_Vectors(df_train.Tweets)\n",
    "yTrain = df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Vector:(300,)\n",
      "Shape of documents(700, 300)\n"
     ]
    }
   ],
   "source": [
    "xTest = Generate_Vectors(df_test.Tweets)\n",
    "yTest = df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- CNN1D ---------------\n",
      "(2800, 1, 300)\n",
      "(700, 1, 300)\n",
      "(1, 300)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1, 32)             28832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 32)             1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 64)             2112      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,065\n",
      "Trainable params: 32,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2240 samples, validate on 560 samples\n",
      "Epoch 1/50\n",
      "2240/2240 [==============================] - 9s 4ms/step - loss: 0.2513 - accuracy: 0.4799 - val_loss: 0.2508 - val_accuracy: 0.4875\n",
      "Epoch 2/50\n",
      "2240/2240 [==============================] - 0s 136us/step - loss: 0.2483 - accuracy: 0.5460 - val_loss: 0.2514 - val_accuracy: 0.4804\n",
      "Epoch 3/50\n",
      "2240/2240 [==============================] - 0s 111us/step - loss: 0.2411 - accuracy: 0.5924 - val_loss: 0.2618 - val_accuracy: 0.4857\n",
      "Epoch 4/50\n",
      "2240/2240 [==============================] - 0s 147us/step - loss: 0.2324 - accuracy: 0.6112 - val_loss: 0.2690 - val_accuracy: 0.5018\n",
      "Epoch 5/50\n",
      "2240/2240 [==============================] - 0s 99us/step - loss: 0.2289 - accuracy: 0.6281 - val_loss: 0.2746 - val_accuracy: 0.5018\n",
      "Epoch 6/50\n",
      "2240/2240 [==============================] - 0s 136us/step - loss: 0.2267 - accuracy: 0.6344 - val_loss: 0.2751 - val_accuracy: 0.4911\n",
      "Epoch 7/50\n",
      "2240/2240 [==============================] - 0s 107us/step - loss: 0.2258 - accuracy: 0.6357 - val_loss: 0.2785 - val_accuracy: 0.4875\n",
      "Epoch 8/50\n",
      "2240/2240 [==============================] - 0s 121us/step - loss: 0.2252 - accuracy: 0.6402 - val_loss: 0.2747 - val_accuracy: 0.4964\n",
      "Epoch 9/50\n",
      "2240/2240 [==============================] - 0s 128us/step - loss: 0.2237 - accuracy: 0.6504 - val_loss: 0.2821 - val_accuracy: 0.4875\n",
      "Epoch 10/50\n",
      "2240/2240 [==============================] - 0s 115us/step - loss: 0.2234 - accuracy: 0.6496 - val_loss: 0.2830 - val_accuracy: 0.4857\n",
      "Epoch 11/50\n",
      "2240/2240 [==============================] - 0s 133us/step - loss: 0.2246 - accuracy: 0.6335 - val_loss: 0.2845 - val_accuracy: 0.4750\n",
      "Epoch 12/50\n",
      "2240/2240 [==============================] - 0s 116us/step - loss: 0.2236 - accuracy: 0.6522 - val_loss: 0.2797 - val_accuracy: 0.4964\n",
      "Epoch 13/50\n",
      "2240/2240 [==============================] - 0s 111us/step - loss: 0.2220 - accuracy: 0.6562 - val_loss: 0.2852 - val_accuracy: 0.4786\n",
      "Epoch 14/50\n",
      "2240/2240 [==============================] - 0s 109us/step - loss: 0.2225 - accuracy: 0.6576 - val_loss: 0.2860 - val_accuracy: 0.4875\n",
      "Epoch 15/50\n",
      "2240/2240 [==============================] - 0s 142us/step - loss: 0.2226 - accuracy: 0.6536 - val_loss: 0.2846 - val_accuracy: 0.4804\n",
      "Epoch 16/50\n",
      "2240/2240 [==============================] - 0s 134us/step - loss: 0.2219 - accuracy: 0.6500 - val_loss: 0.2879 - val_accuracy: 0.4786\n",
      "Epoch 17/50\n",
      "2240/2240 [==============================] - 0s 130us/step - loss: 0.2225 - accuracy: 0.6513 - val_loss: 0.2870 - val_accuracy: 0.4839\n",
      "Epoch 18/50\n",
      "2240/2240 [==============================] - 0s 116us/step - loss: 0.2216 - accuracy: 0.6536 - val_loss: 0.2856 - val_accuracy: 0.4786\n",
      "Epoch 19/50\n",
      "2240/2240 [==============================] - 0s 100us/step - loss: 0.2211 - accuracy: 0.6567 - val_loss: 0.2883 - val_accuracy: 0.4857\n",
      "Epoch 20/50\n",
      "2240/2240 [==============================] - 0s 124us/step - loss: 0.2220 - accuracy: 0.6540 - val_loss: 0.2810 - val_accuracy: 0.4946\n",
      "Epoch 21/50\n",
      "2240/2240 [==============================] - 0s 135us/step - loss: 0.2210 - accuracy: 0.6621 - val_loss: 0.2905 - val_accuracy: 0.4893\n",
      "Epoch 22/50\n",
      "2240/2240 [==============================] - 0s 115us/step - loss: 0.2204 - accuracy: 0.6562 - val_loss: 0.2898 - val_accuracy: 0.4839\n",
      "Epoch 23/50\n",
      "2240/2240 [==============================] - 0s 85us/step - loss: 0.2217 - accuracy: 0.6545 - val_loss: 0.2859 - val_accuracy: 0.4821\n",
      "Epoch 24/50\n",
      "2240/2240 [==============================] - 0s 135us/step - loss: 0.2213 - accuracy: 0.6612 - val_loss: 0.2862 - val_accuracy: 0.4821\n",
      "Epoch 25/50\n",
      "2240/2240 [==============================] - 0s 96us/step - loss: 0.2209 - accuracy: 0.6652 - val_loss: 0.2881 - val_accuracy: 0.4750\n",
      "Epoch 26/50\n",
      "2240/2240 [==============================] - 0s 114us/step - loss: 0.2205 - accuracy: 0.6607 - val_loss: 0.2867 - val_accuracy: 0.4750\n",
      "Epoch 27/50\n",
      "2240/2240 [==============================] - 0s 112us/step - loss: 0.2217 - accuracy: 0.6580 - val_loss: 0.2903 - val_accuracy: 0.4804\n",
      "Epoch 28/50\n",
      "2240/2240 [==============================] - 0s 124us/step - loss: 0.2210 - accuracy: 0.6625 - val_loss: 0.2920 - val_accuracy: 0.4768\n",
      "Epoch 29/50\n",
      "2240/2240 [==============================] - 0s 119us/step - loss: 0.2212 - accuracy: 0.6607 - val_loss: 0.2849 - val_accuracy: 0.4946\n",
      "Epoch 30/50\n",
      "2240/2240 [==============================] - 0s 132us/step - loss: 0.2215 - accuracy: 0.6621 - val_loss: 0.2901 - val_accuracy: 0.4839\n",
      "Epoch 31/50\n",
      "2240/2240 [==============================] - 0s 136us/step - loss: 0.2203 - accuracy: 0.6683 - val_loss: 0.2929 - val_accuracy: 0.4821\n",
      "Epoch 32/50\n",
      "2240/2240 [==============================] - 0s 127us/step - loss: 0.2207 - accuracy: 0.6643 - val_loss: 0.2858 - val_accuracy: 0.4929\n",
      "Epoch 33/50\n",
      "2240/2240 [==============================] - 0s 127us/step - loss: 0.2200 - accuracy: 0.6696 - val_loss: 0.2914 - val_accuracy: 0.4750\n",
      "Epoch 34/50\n",
      "2240/2240 [==============================] - 0s 128us/step - loss: 0.2218 - accuracy: 0.6598 - val_loss: 0.2871 - val_accuracy: 0.4982\n",
      "Epoch 35/50\n",
      "2240/2240 [==============================] - 0s 84us/step - loss: 0.2205 - accuracy: 0.6638 - val_loss: 0.2841 - val_accuracy: 0.4964\n",
      "Epoch 36/50\n",
      "2240/2240 [==============================] - 0s 142us/step - loss: 0.2206 - accuracy: 0.6674 - val_loss: 0.2885 - val_accuracy: 0.4804\n",
      "Epoch 37/50\n",
      "2240/2240 [==============================] - 0s 91us/step - loss: 0.2213 - accuracy: 0.6679 - val_loss: 0.2926 - val_accuracy: 0.4857\n",
      "Epoch 38/50\n",
      "2240/2240 [==============================] - 0s 120us/step - loss: 0.2192 - accuracy: 0.6687 - val_loss: 0.2917 - val_accuracy: 0.4875\n",
      "Epoch 39/50\n",
      "2240/2240 [==============================] - 0s 114us/step - loss: 0.2196 - accuracy: 0.6687 - val_loss: 0.2906 - val_accuracy: 0.4839\n",
      "Epoch 40/50\n",
      "2240/2240 [==============================] - 0s 134us/step - loss: 0.2206 - accuracy: 0.6634 - val_loss: 0.2894 - val_accuracy: 0.4804\n",
      "Epoch 41/50\n",
      "2240/2240 [==============================] - 0s 137us/step - loss: 0.2200 - accuracy: 0.6701 - val_loss: 0.2920 - val_accuracy: 0.4929\n",
      "Epoch 42/50\n",
      "2240/2240 [==============================] - 0s 121us/step - loss: 0.2210 - accuracy: 0.6647 - val_loss: 0.2869 - val_accuracy: 0.4893\n",
      "Epoch 43/50\n",
      "2240/2240 [==============================] - 0s 138us/step - loss: 0.2195 - accuracy: 0.6723 - val_loss: 0.2975 - val_accuracy: 0.4750\n",
      "Epoch 44/50\n",
      "2240/2240 [==============================] - 0s 121us/step - loss: 0.2197 - accuracy: 0.6652 - val_loss: 0.2940 - val_accuracy: 0.4839\n",
      "Epoch 45/50\n",
      "2240/2240 [==============================] - 0s 118us/step - loss: 0.2203 - accuracy: 0.6687 - val_loss: 0.2925 - val_accuracy: 0.4839\n",
      "Epoch 46/50\n",
      "2240/2240 [==============================] - 0s 117us/step - loss: 0.2196 - accuracy: 0.6705 - val_loss: 0.2939 - val_accuracy: 0.4768\n",
      "Epoch 47/50\n",
      "2240/2240 [==============================] - 0s 127us/step - loss: 0.2190 - accuracy: 0.6696 - val_loss: 0.2872 - val_accuracy: 0.4857\n",
      "Epoch 48/50\n",
      "2240/2240 [==============================] - 0s 123us/step - loss: 0.2207 - accuracy: 0.6674 - val_loss: 0.2960 - val_accuracy: 0.4857\n",
      "Epoch 49/50\n",
      "2240/2240 [==============================] - 0s 98us/step - loss: 0.2196 - accuracy: 0.6732 - val_loss: 0.2947 - val_accuracy: 0.4786\n",
      "Epoch 50/50\n",
      "2240/2240 [==============================] - 0s 121us/step - loss: 0.2197 - accuracy: 0.6679 - val_loss: 0.2892 - val_accuracy: 0.4821\n",
      "[0.269637336560658, 0.5199999809265137]\n",
      "=== Classification Report Conv 1D ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       350\n",
      "           1       0.52      0.52      0.52       350\n",
      "\n",
      "    accuracy                           0.52       700\n",
      "   macro avg       0.52      0.52      0.52       700\n",
      "weighted avg       0.52      0.52      0.52       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix Conv 1D ===\n",
      "[[182 168]\n",
      " [168 182]]\n",
      "\n",
      "\n",
      "--------------- CNN1D ---------------\n"
     ]
    }
   ],
   "source": [
    "cnn1d_ngram = CNN1D_Ngrams(xTrain, xTest, yTrain, yTest,_epochs = 50, _verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- LSTM ---------------\n",
      "(2800, 1, 300)\n",
      "(700, 1, 300)\n",
      "(1, 300)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1, 4)              4880      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 4,885\n",
      "Trainable params: 4,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2240 samples, validate on 560 samples\n",
      "Epoch 1/50\n",
      "2240/2240 [==============================] - 1s 438us/step - loss: 0.2501 - accuracy: 0.4978 - val_loss: 0.2501 - val_accuracy: 0.5054\n",
      "Epoch 2/50\n",
      "2240/2240 [==============================] - 0s 160us/step - loss: 0.2496 - accuracy: 0.5080 - val_loss: 0.2502 - val_accuracy: 0.4964\n",
      "Epoch 3/50\n",
      "2240/2240 [==============================] - 0s 176us/step - loss: 0.2490 - accuracy: 0.5420 - val_loss: 0.2502 - val_accuracy: 0.5107\n",
      "Epoch 4/50\n",
      "2240/2240 [==============================] - 0s 168us/step - loss: 0.2486 - accuracy: 0.5446 - val_loss: 0.2504 - val_accuracy: 0.5089\n",
      "Epoch 5/50\n",
      "2240/2240 [==============================] - 0s 161us/step - loss: 0.2477 - accuracy: 0.5705 - val_loss: 0.2504 - val_accuracy: 0.4982\n",
      "Epoch 6/50\n",
      "2240/2240 [==============================] - 0s 171us/step - loss: 0.2470 - accuracy: 0.5777 - val_loss: 0.2505 - val_accuracy: 0.5036\n",
      "Epoch 7/50\n",
      "2240/2240 [==============================] - 0s 171us/step - loss: 0.2457 - accuracy: 0.5964 - val_loss: 0.2508 - val_accuracy: 0.4946\n",
      "Epoch 8/50\n",
      "2240/2240 [==============================] - 0s 169us/step - loss: 0.2442 - accuracy: 0.5991 - val_loss: 0.2510 - val_accuracy: 0.4964\n",
      "Epoch 9/50\n",
      "2240/2240 [==============================] - 0s 160us/step - loss: 0.2422 - accuracy: 0.6121 - val_loss: 0.2514 - val_accuracy: 0.4804\n",
      "Epoch 10/50\n",
      "2240/2240 [==============================] - 0s 198us/step - loss: 0.2402 - accuracy: 0.6210 - val_loss: 0.2520 - val_accuracy: 0.4821\n",
      "Epoch 11/50\n",
      "2240/2240 [==============================] - 0s 195us/step - loss: 0.2382 - accuracy: 0.6219 - val_loss: 0.2527 - val_accuracy: 0.4786\n",
      "Epoch 12/50\n",
      "2240/2240 [==============================] - 0s 174us/step - loss: 0.2356 - accuracy: 0.6250 - val_loss: 0.2536 - val_accuracy: 0.4786\n",
      "Epoch 13/50\n",
      "2240/2240 [==============================] - 0s 117us/step - loss: 0.2335 - accuracy: 0.6299 - val_loss: 0.2548 - val_accuracy: 0.4804\n",
      "Epoch 14/50\n",
      "2240/2240 [==============================] - 0s 181us/step - loss: 0.2305 - accuracy: 0.6330 - val_loss: 0.2564 - val_accuracy: 0.4875\n",
      "Epoch 15/50\n",
      "2240/2240 [==============================] - 0s 158us/step - loss: 0.2280 - accuracy: 0.6442 - val_loss: 0.2579 - val_accuracy: 0.4946\n",
      "Epoch 16/50\n",
      "2240/2240 [==============================] - 0s 157us/step - loss: 0.2264 - accuracy: 0.6362 - val_loss: 0.2593 - val_accuracy: 0.5036\n",
      "Epoch 17/50\n",
      "2240/2240 [==============================] - 0s 162us/step - loss: 0.2256 - accuracy: 0.6397 - val_loss: 0.2610 - val_accuracy: 0.5071\n",
      "Epoch 18/50\n",
      "2240/2240 [==============================] - 0s 182us/step - loss: 0.2229 - accuracy: 0.6482 - val_loss: 0.2632 - val_accuracy: 0.5089\n",
      "Epoch 19/50\n",
      "2240/2240 [==============================] - 0s 182us/step - loss: 0.2226 - accuracy: 0.6424 - val_loss: 0.2641 - val_accuracy: 0.5107\n",
      "Epoch 20/50\n",
      "2240/2240 [==============================] - 0s 164us/step - loss: 0.2213 - accuracy: 0.6406 - val_loss: 0.2658 - val_accuracy: 0.5071\n",
      "Epoch 21/50\n",
      "2240/2240 [==============================] - 0s 170us/step - loss: 0.2198 - accuracy: 0.6415 - val_loss: 0.2668 - val_accuracy: 0.4982\n",
      "Epoch 22/50\n",
      "2240/2240 [==============================] - 0s 154us/step - loss: 0.2186 - accuracy: 0.6482 - val_loss: 0.2689 - val_accuracy: 0.4929\n",
      "Epoch 23/50\n",
      "2240/2240 [==============================] - 0s 153us/step - loss: 0.2173 - accuracy: 0.6536 - val_loss: 0.2704 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "2240/2240 [==============================] - 0s 194us/step - loss: 0.2178 - accuracy: 0.6576 - val_loss: 0.2714 - val_accuracy: 0.4982\n",
      "Epoch 25/50\n",
      "2240/2240 [==============================] - 0s 170us/step - loss: 0.2159 - accuracy: 0.6576 - val_loss: 0.2727 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "2240/2240 [==============================] - 0s 165us/step - loss: 0.2159 - accuracy: 0.6554 - val_loss: 0.2733 - val_accuracy: 0.4964\n",
      "Epoch 27/50\n",
      "2240/2240 [==============================] - 0s 187us/step - loss: 0.2155 - accuracy: 0.6540 - val_loss: 0.2745 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "2240/2240 [==============================] - 0s 165us/step - loss: 0.2149 - accuracy: 0.6531 - val_loss: 0.2754 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "2240/2240 [==============================] - 0s 181us/step - loss: 0.2146 - accuracy: 0.6616 - val_loss: 0.2762 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "2240/2240 [==============================] - 0s 167us/step - loss: 0.2140 - accuracy: 0.6589 - val_loss: 0.2767 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "2240/2240 [==============================] - 0s 174us/step - loss: 0.2141 - accuracy: 0.6585 - val_loss: 0.2775 - val_accuracy: 0.5018\n",
      "Epoch 32/50\n",
      "2240/2240 [==============================] - 0s 168us/step - loss: 0.2130 - accuracy: 0.6656 - val_loss: 0.2779 - val_accuracy: 0.4946\n",
      "Epoch 33/50\n",
      "2240/2240 [==============================] - 0s 168us/step - loss: 0.2134 - accuracy: 0.6687 - val_loss: 0.2786 - val_accuracy: 0.4982\n",
      "Epoch 34/50\n",
      "2240/2240 [==============================] - 0s 139us/step - loss: 0.2104 - accuracy: 0.6674 - val_loss: 0.2793 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "2240/2240 [==============================] - 0s 163us/step - loss: 0.2107 - accuracy: 0.6683 - val_loss: 0.2795 - val_accuracy: 0.4964\n",
      "Epoch 36/50\n",
      "2240/2240 [==============================] - 0s 179us/step - loss: 0.2124 - accuracy: 0.6625 - val_loss: 0.2798 - val_accuracy: 0.4964\n",
      "Epoch 37/50\n",
      "2240/2240 [==============================] - 0s 173us/step - loss: 0.2103 - accuracy: 0.6571 - val_loss: 0.2803 - val_accuracy: 0.4964\n",
      "Epoch 38/50\n",
      "2240/2240 [==============================] - 0s 176us/step - loss: 0.2094 - accuracy: 0.6621 - val_loss: 0.2812 - val_accuracy: 0.4982\n",
      "Epoch 39/50\n",
      "2240/2240 [==============================] - 0s 186us/step - loss: 0.2085 - accuracy: 0.6714 - val_loss: 0.2815 - val_accuracy: 0.5036\n",
      "Epoch 40/50\n",
      "2240/2240 [==============================] - 0s 185us/step - loss: 0.2079 - accuracy: 0.6737 - val_loss: 0.2818 - val_accuracy: 0.5018\n",
      "Epoch 41/50\n",
      "2240/2240 [==============================] - 0s 163us/step - loss: 0.2073 - accuracy: 0.6696 - val_loss: 0.2819 - val_accuracy: 0.4982\n",
      "Epoch 42/50\n",
      "2240/2240 [==============================] - 0s 171us/step - loss: 0.2081 - accuracy: 0.6732 - val_loss: 0.2822 - val_accuracy: 0.4982\n",
      "Epoch 43/50\n",
      "2240/2240 [==============================] - 0s 156us/step - loss: 0.2061 - accuracy: 0.6705 - val_loss: 0.2826 - val_accuracy: 0.5018\n",
      "Epoch 44/50\n",
      "2240/2240 [==============================] - 0s 153us/step - loss: 0.2066 - accuracy: 0.6746 - val_loss: 0.2830 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "2240/2240 [==============================] - 0s 189us/step - loss: 0.2071 - accuracy: 0.6692 - val_loss: 0.2831 - val_accuracy: 0.4964\n",
      "Epoch 46/50\n",
      "2240/2240 [==============================] - 0s 139us/step - loss: 0.2061 - accuracy: 0.6826 - val_loss: 0.2835 - val_accuracy: 0.4964\n",
      "Epoch 47/50\n",
      "2240/2240 [==============================] - 0s 169us/step - loss: 0.2030 - accuracy: 0.6817 - val_loss: 0.2839 - val_accuracy: 0.4982\n",
      "Epoch 48/50\n",
      "2240/2240 [==============================] - 0s 166us/step - loss: 0.2042 - accuracy: 0.6763 - val_loss: 0.2839 - val_accuracy: 0.4982\n",
      "Epoch 49/50\n",
      "2240/2240 [==============================] - 0s 168us/step - loss: 0.2033 - accuracy: 0.6830 - val_loss: 0.2843 - val_accuracy: 0.4982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "2240/2240 [==============================] - 0s 146us/step - loss: 0.2020 - accuracy: 0.6844 - val_loss: 0.2847 - val_accuracy: 0.4964\n",
      "=== AUC Score ===\n",
      "Accuracy: 0.531429\n",
      "=== Classification Report LSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.54      0.53       350\n",
      "           1       0.53      0.53      0.53       350\n",
      "\n",
      "    accuracy                           0.53       700\n",
      "   macro avg       0.53      0.53      0.53       700\n",
      "weighted avg       0.53      0.53      0.53       700\n",
      "\n",
      "\n",
      "\n",
      "=== Confusion Matrix LSTM ===\n",
      "[[188 162]\n",
      " [166 184]]\n",
      "\n",
      "\n",
      "=== ROC Score Naive Bayes ===\n",
      "0.5314285714285714\n",
      "--------------- LSTM ---------------\n"
     ]
    }
   ],
   "source": [
    "lstm_ngram = LSTM_Ngrams(xTrain, xTest, yTrain, yTest,_epochs = 50, _verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
